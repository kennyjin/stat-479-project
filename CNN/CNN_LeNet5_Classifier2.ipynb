{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 123\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Architecture\n",
    "NUM_FEATURES = 28*28\n",
    "NUM_CLASSES = 345\n",
    "\n",
    "# Other\n",
    "DEVICE = \"cuda:0\"\n",
    "GRAYSCALE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "images = np.load(\"Data/1000/sample_1000_image.npy\")\n",
    "labels = np.load(\"Data/1000/sample_1000_label.npy\")\n",
    "\n",
    "# Normalize image data.  0-255 to 0-1\n",
    "images = images / 255\n",
    "df = pd.DataFrame(np.concatenate((images, labels), axis=1))\n",
    "\n",
    "# Rename the last column as \"label\"\n",
    "df.rename(columns={784:\"label\"}, inplace=True)\n",
    "\n",
    "# Convert label column to integer type\n",
    "df['label'] = df['label'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get \"img\" data frame and \"lbl\" series from the df\n",
    "img = df.iloc[:, 0:-1]\n",
    "lbl = df['label']\n",
    "# Split into train, validation and test set\n",
    "x_train1, x_test, y_train1, y_test = train_test_split(img, lbl, test_size = 0.10, random_state = 123, stratify = lbl)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train1, y_train1, test_size = 0.2, random_state = 123, stratify = y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(248400, 784)\n",
      "(62100, 784)\n",
      "(34500, 784)\n",
      "(248400,)\n",
      "(62100,)\n",
      "(34500,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to tensor\n",
    "x_train = torch.tensor(x_train.values)\n",
    "y_train = torch.tensor(y_train.values)\n",
    "\n",
    "x_valid = torch.tensor(x_valid.values)\n",
    "y_valid = torch.tensor(y_valid.values)\n",
    "\n",
    "x_test = torch.tensor(x_test.values)\n",
    "y_test = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape all tensors to the size of (length, 1, 28, 28)\n",
    "x_train = x_train.reshape((-1, 1, 28, 28))\n",
    "x_valid = x_valid.reshape((-1, 1, 28, 28))\n",
    "x_test = x_test.reshape((-1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True, num_workers=4)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=False, num_workers=4)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([64, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batch index: 0 | Batch size: 64\n",
      "Epoch: 2 | Batch index: 0 | Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        print('Epoch:', epoch+1, end='')\n",
    "        print(' | Batch index:', batch_idx, end='')\n",
    "        print(' | Batch size:', y.size()[0])\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, grayscale=False):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.grayscale = grayscale\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if self.grayscale:\n",
    "            in_channels = 1\n",
    "        else:\n",
    "            in_channels = 3\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_channels, 6, kernel_size=5),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Linear(84, num_classes),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = LeNet5(NUM_CLASSES, GRAYSCALE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sizes(self, input, output):\n",
    "\n",
    "    print('Inside ' + self.__class__.__name__ + ' forward')\n",
    "    print('input size:', input[0].size())\n",
    "    print('output size:', output.data.size())\n",
    "\n",
    "    \n",
    "## Debugging\n",
    "\n",
    "\n",
    "# model.features[0].register_forward_hook(print_sizes)\n",
    "# model.features[1].register_forward_hook(print_sizes)\n",
    "# model.features[2].register_forward_hook(print_sizes)\n",
    "# model.features[3].register_forward_hook(print_sizes)\n",
    "\n",
    "# model.classifier[0].register_forward_hook(print_sizes)\n",
    "# model.classifier[1].register_forward_hook(print_sizes)\n",
    "# model.classifier[2].register_forward_hook(print_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "        \n",
    "        features = features.float()\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Batch 0000/3882 | Cost: 5.8429\n",
      "Epoch: 001/050 | Batch 0500/3882 | Cost: 4.3395\n",
      "Epoch: 001/050 | Batch 1000/3882 | Cost: 4.2463\n",
      "Epoch: 001/050 | Batch 1500/3882 | Cost: 3.5139\n",
      "Epoch: 001/050 | Batch 2000/3882 | Cost: 3.6813\n",
      "Epoch: 001/050 | Batch 2500/3882 | Cost: 3.8267\n",
      "Epoch: 001/050 | Batch 3000/3882 | Cost: 3.2623\n",
      "Epoch: 001/050 | Batch 3500/3882 | Cost: 3.5534\n",
      "Epoch: 001/050 | Train: 31.184% | Validation: 30.712%\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 002/050 | Batch 0000/3882 | Cost: 3.0288\n",
      "Epoch: 002/050 | Batch 0500/3882 | Cost: 3.2172\n",
      "Epoch: 002/050 | Batch 1000/3882 | Cost: 2.8994\n",
      "Epoch: 002/050 | Batch 1500/3882 | Cost: 3.4659\n",
      "Epoch: 002/050 | Batch 2000/3882 | Cost: 3.3334\n",
      "Epoch: 002/050 | Batch 2500/3882 | Cost: 2.9032\n",
      "Epoch: 002/050 | Batch 3000/3882 | Cost: 3.0670\n",
      "Epoch: 002/050 | Batch 3500/3882 | Cost: 3.1190\n",
      "Epoch: 002/050 | Train: 36.081% | Validation: 35.438%\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 003/050 | Batch 0000/3882 | Cost: 2.4372\n",
      "Epoch: 003/050 | Batch 0500/3882 | Cost: 2.9497\n",
      "Epoch: 003/050 | Batch 1000/3882 | Cost: 3.9470\n",
      "Epoch: 003/050 | Batch 1500/3882 | Cost: 3.1111\n",
      "Epoch: 003/050 | Batch 2000/3882 | Cost: 2.8766\n",
      "Epoch: 003/050 | Batch 2500/3882 | Cost: 2.5496\n",
      "Epoch: 003/050 | Batch 3000/3882 | Cost: 3.0910\n",
      "Epoch: 003/050 | Batch 3500/3882 | Cost: 2.8412\n",
      "Epoch: 003/050 | Train: 38.492% | Validation: 37.721%\n",
      "Time elapsed: 1.46 min\n",
      "Epoch: 004/050 | Batch 0000/3882 | Cost: 2.7315\n",
      "Epoch: 004/050 | Batch 0500/3882 | Cost: 3.1924\n",
      "Epoch: 004/050 | Batch 1000/3882 | Cost: 3.2318\n",
      "Epoch: 004/050 | Batch 1500/3882 | Cost: 3.0679\n",
      "Epoch: 004/050 | Batch 2000/3882 | Cost: 2.9257\n",
      "Epoch: 004/050 | Batch 2500/3882 | Cost: 2.8879\n",
      "Epoch: 004/050 | Batch 3000/3882 | Cost: 3.0126\n",
      "Epoch: 004/050 | Batch 3500/3882 | Cost: 2.6632\n",
      "Epoch: 004/050 | Train: 40.484% | Validation: 39.556%\n",
      "Time elapsed: 1.95 min\n",
      "Epoch: 005/050 | Batch 0000/3882 | Cost: 2.3918\n",
      "Epoch: 005/050 | Batch 0500/3882 | Cost: 2.8970\n",
      "Epoch: 005/050 | Batch 1000/3882 | Cost: 3.0235\n",
      "Epoch: 005/050 | Batch 1500/3882 | Cost: 2.4436\n",
      "Epoch: 005/050 | Batch 2000/3882 | Cost: 2.6507\n",
      "Epoch: 005/050 | Batch 2500/3882 | Cost: 2.5148\n",
      "Epoch: 005/050 | Batch 3000/3882 | Cost: 2.3799\n",
      "Epoch: 005/050 | Batch 3500/3882 | Cost: 2.9437\n",
      "Epoch: 005/050 | Train: 41.466% | Validation: 40.441%\n",
      "Time elapsed: 2.44 min\n",
      "Epoch: 006/050 | Batch 0000/3882 | Cost: 2.5462\n",
      "Epoch: 006/050 | Batch 0500/3882 | Cost: 2.5157\n",
      "Epoch: 006/050 | Batch 1000/3882 | Cost: 2.9182\n",
      "Epoch: 006/050 | Batch 1500/3882 | Cost: 2.4961\n",
      "Epoch: 006/050 | Batch 2000/3882 | Cost: 2.9476\n",
      "Epoch: 006/050 | Batch 2500/3882 | Cost: 2.4175\n",
      "Epoch: 006/050 | Batch 3000/3882 | Cost: 3.0481\n",
      "Epoch: 006/050 | Batch 3500/3882 | Cost: 2.6265\n",
      "Epoch: 006/050 | Train: 42.710% | Validation: 41.605%\n",
      "Time elapsed: 2.92 min\n",
      "Epoch: 007/050 | Batch 0000/3882 | Cost: 2.7682\n",
      "Epoch: 007/050 | Batch 0500/3882 | Cost: 2.1280\n",
      "Epoch: 007/050 | Batch 1000/3882 | Cost: 2.5013\n",
      "Epoch: 007/050 | Batch 1500/3882 | Cost: 3.3275\n",
      "Epoch: 007/050 | Batch 2000/3882 | Cost: 2.5426\n",
      "Epoch: 007/050 | Batch 2500/3882 | Cost: 2.5799\n",
      "Epoch: 007/050 | Batch 3000/3882 | Cost: 3.2516\n",
      "Epoch: 007/050 | Batch 3500/3882 | Cost: 2.0551\n",
      "Epoch: 007/050 | Train: 43.773% | Validation: 42.494%\n",
      "Time elapsed: 3.41 min\n",
      "Epoch: 008/050 | Batch 0000/3882 | Cost: 2.9066\n",
      "Epoch: 008/050 | Batch 0500/3882 | Cost: 3.2670\n",
      "Epoch: 008/050 | Batch 1000/3882 | Cost: 2.1408\n",
      "Epoch: 008/050 | Batch 1500/3882 | Cost: 1.9659\n",
      "Epoch: 008/050 | Batch 2000/3882 | Cost: 2.2648\n",
      "Epoch: 008/050 | Batch 2500/3882 | Cost: 3.1787\n",
      "Epoch: 008/050 | Batch 3000/3882 | Cost: 2.7682\n",
      "Epoch: 008/050 | Batch 3500/3882 | Cost: 2.5788\n",
      "Epoch: 008/050 | Train: 44.513% | Validation: 43.003%\n",
      "Time elapsed: 3.91 min\n",
      "Epoch: 009/050 | Batch 0000/3882 | Cost: 2.2751\n",
      "Epoch: 009/050 | Batch 0500/3882 | Cost: 2.4702\n",
      "Epoch: 009/050 | Batch 1000/3882 | Cost: 2.9526\n",
      "Epoch: 009/050 | Batch 1500/3882 | Cost: 2.2991\n",
      "Epoch: 009/050 | Batch 2000/3882 | Cost: 2.5234\n",
      "Epoch: 009/050 | Batch 2500/3882 | Cost: 2.6894\n",
      "Epoch: 009/050 | Batch 3000/3882 | Cost: 2.8220\n",
      "Epoch: 009/050 | Batch 3500/3882 | Cost: 2.9395\n",
      "Epoch: 009/050 | Train: 44.782% | Validation: 43.132%\n",
      "Time elapsed: 4.40 min\n",
      "Epoch: 010/050 | Batch 0000/3882 | Cost: 2.5300\n",
      "Epoch: 010/050 | Batch 0500/3882 | Cost: 3.0432\n",
      "Epoch: 010/050 | Batch 1000/3882 | Cost: 2.4293\n",
      "Epoch: 010/050 | Batch 1500/3882 | Cost: 2.9073\n",
      "Epoch: 010/050 | Batch 2000/3882 | Cost: 3.0199\n",
      "Epoch: 010/050 | Batch 2500/3882 | Cost: 2.7680\n",
      "Epoch: 010/050 | Batch 3000/3882 | Cost: 2.6988\n",
      "Epoch: 010/050 | Batch 3500/3882 | Cost: 2.9278\n",
      "Epoch: 010/050 | Train: 45.241% | Validation: 43.295%\n",
      "Time elapsed: 4.88 min\n",
      "Epoch: 011/050 | Batch 0000/3882 | Cost: 2.3571\n",
      "Epoch: 011/050 | Batch 0500/3882 | Cost: 2.7023\n",
      "Epoch: 011/050 | Batch 1000/3882 | Cost: 2.9097\n",
      "Epoch: 011/050 | Batch 1500/3882 | Cost: 2.7149\n",
      "Epoch: 011/050 | Batch 2000/3882 | Cost: 2.2169\n",
      "Epoch: 011/050 | Batch 2500/3882 | Cost: 2.3371\n",
      "Epoch: 011/050 | Batch 3000/3882 | Cost: 2.1912\n",
      "Epoch: 011/050 | Batch 3500/3882 | Cost: 2.5009\n",
      "Epoch: 011/050 | Train: 45.820% | Validation: 44.058%\n",
      "Time elapsed: 5.37 min\n",
      "Epoch: 012/050 | Batch 0000/3882 | Cost: 2.2879\n",
      "Epoch: 012/050 | Batch 0500/3882 | Cost: 2.7391\n",
      "Epoch: 012/050 | Batch 1000/3882 | Cost: 2.2821\n",
      "Epoch: 012/050 | Batch 1500/3882 | Cost: 2.7638\n",
      "Epoch: 012/050 | Batch 2000/3882 | Cost: 2.1532\n",
      "Epoch: 012/050 | Batch 2500/3882 | Cost: 2.4294\n",
      "Epoch: 012/050 | Batch 3000/3882 | Cost: 2.2386\n",
      "Epoch: 012/050 | Batch 3500/3882 | Cost: 2.6221\n",
      "Epoch: 012/050 | Train: 46.454% | Validation: 44.496%\n",
      "Time elapsed: 5.86 min\n",
      "Epoch: 013/050 | Batch 0000/3882 | Cost: 2.4880\n",
      "Epoch: 013/050 | Batch 0500/3882 | Cost: 2.6119\n",
      "Epoch: 013/050 | Batch 1000/3882 | Cost: 2.3689\n",
      "Epoch: 013/050 | Batch 1500/3882 | Cost: 2.2131\n",
      "Epoch: 013/050 | Batch 2000/3882 | Cost: 2.8363\n",
      "Epoch: 013/050 | Batch 2500/3882 | Cost: 2.0592\n",
      "Epoch: 013/050 | Batch 3000/3882 | Cost: 2.2309\n",
      "Epoch: 013/050 | Batch 3500/3882 | Cost: 2.4458\n",
      "Epoch: 013/050 | Train: 46.587% | Validation: 44.758%\n",
      "Time elapsed: 6.35 min\n",
      "Epoch: 014/050 | Batch 0000/3882 | Cost: 2.4408\n",
      "Epoch: 014/050 | Batch 0500/3882 | Cost: 2.6696\n",
      "Epoch: 014/050 | Batch 1000/3882 | Cost: 2.5953\n",
      "Epoch: 014/050 | Batch 1500/3882 | Cost: 2.3355\n",
      "Epoch: 014/050 | Batch 2000/3882 | Cost: 2.5258\n",
      "Epoch: 014/050 | Batch 2500/3882 | Cost: 2.3963\n",
      "Epoch: 014/050 | Batch 3000/3882 | Cost: 2.7526\n",
      "Epoch: 014/050 | Batch 3500/3882 | Cost: 2.3662\n",
      "Epoch: 014/050 | Train: 46.874% | Validation: 44.990%\n",
      "Time elapsed: 6.85 min\n",
      "Epoch: 015/050 | Batch 0000/3882 | Cost: 2.6807\n",
      "Epoch: 015/050 | Batch 0500/3882 | Cost: 2.4389\n",
      "Epoch: 015/050 | Batch 1000/3882 | Cost: 2.3001\n",
      "Epoch: 015/050 | Batch 1500/3882 | Cost: 2.4150\n",
      "Epoch: 015/050 | Batch 2000/3882 | Cost: 2.4114\n",
      "Epoch: 015/050 | Batch 2500/3882 | Cost: 2.4823\n",
      "Epoch: 015/050 | Batch 3000/3882 | Cost: 2.5047\n",
      "Epoch: 015/050 | Batch 3500/3882 | Cost: 2.1202\n",
      "Epoch: 015/050 | Train: 46.821% | Validation: 44.663%\n",
      "Time elapsed: 7.34 min\n",
      "Epoch: 016/050 | Batch 0000/3882 | Cost: 2.5662\n",
      "Epoch: 016/050 | Batch 0500/3882 | Cost: 2.2766\n",
      "Epoch: 016/050 | Batch 1000/3882 | Cost: 2.5534\n",
      "Epoch: 016/050 | Batch 1500/3882 | Cost: 2.7046\n",
      "Epoch: 016/050 | Batch 2000/3882 | Cost: 2.5145\n",
      "Epoch: 016/050 | Batch 2500/3882 | Cost: 2.4779\n",
      "Epoch: 016/050 | Batch 3000/3882 | Cost: 2.5980\n",
      "Epoch: 016/050 | Batch 3500/3882 | Cost: 2.4307\n",
      "Epoch: 016/050 | Train: 47.481% | Validation: 45.311%\n",
      "Time elapsed: 7.83 min\n",
      "Epoch: 017/050 | Batch 0000/3882 | Cost: 2.0362\n",
      "Epoch: 017/050 | Batch 0500/3882 | Cost: 2.3618\n",
      "Epoch: 017/050 | Batch 1000/3882 | Cost: 2.5233\n",
      "Epoch: 017/050 | Batch 1500/3882 | Cost: 2.5441\n",
      "Epoch: 017/050 | Batch 2000/3882 | Cost: 2.7070\n",
      "Epoch: 017/050 | Batch 2500/3882 | Cost: 2.2850\n",
      "Epoch: 017/050 | Batch 3000/3882 | Cost: 2.6122\n",
      "Epoch: 017/050 | Batch 3500/3882 | Cost: 2.2428\n",
      "Epoch: 017/050 | Train: 47.440% | Validation: 45.166%\n",
      "Time elapsed: 8.32 min\n",
      "Epoch: 018/050 | Batch 0000/3882 | Cost: 2.4151\n",
      "Epoch: 018/050 | Batch 0500/3882 | Cost: 2.7553\n",
      "Epoch: 018/050 | Batch 1000/3882 | Cost: 2.3885\n",
      "Epoch: 018/050 | Batch 1500/3882 | Cost: 2.2201\n",
      "Epoch: 018/050 | Batch 2000/3882 | Cost: 2.4800\n",
      "Epoch: 018/050 | Batch 2500/3882 | Cost: 2.9361\n",
      "Epoch: 018/050 | Batch 3000/3882 | Cost: 2.5563\n",
      "Epoch: 018/050 | Batch 3500/3882 | Cost: 2.8589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018/050 | Train: 47.760% | Validation: 45.412%\n",
      "Time elapsed: 8.81 min\n",
      "Epoch: 019/050 | Batch 0000/3882 | Cost: 1.8553\n",
      "Epoch: 019/050 | Batch 0500/3882 | Cost: 2.7019\n",
      "Epoch: 019/050 | Batch 1000/3882 | Cost: 2.0054\n",
      "Epoch: 019/050 | Batch 1500/3882 | Cost: 3.0068\n",
      "Epoch: 019/050 | Batch 2000/3882 | Cost: 2.3781\n",
      "Epoch: 019/050 | Batch 2500/3882 | Cost: 2.3243\n",
      "Epoch: 019/050 | Batch 3000/3882 | Cost: 2.0548\n",
      "Epoch: 019/050 | Batch 3500/3882 | Cost: 2.3907\n",
      "Epoch: 019/050 | Train: 47.925% | Validation: 45.596%\n",
      "Time elapsed: 9.29 min\n",
      "Epoch: 020/050 | Batch 0000/3882 | Cost: 2.5784\n",
      "Epoch: 020/050 | Batch 0500/3882 | Cost: 2.3957\n",
      "Epoch: 020/050 | Batch 1000/3882 | Cost: 2.5554\n",
      "Epoch: 020/050 | Batch 1500/3882 | Cost: 2.5139\n",
      "Epoch: 020/050 | Batch 2000/3882 | Cost: 2.2983\n",
      "Epoch: 020/050 | Batch 2500/3882 | Cost: 2.8447\n",
      "Epoch: 020/050 | Batch 3000/3882 | Cost: 2.6835\n",
      "Epoch: 020/050 | Batch 3500/3882 | Cost: 2.5994\n",
      "Epoch: 020/050 | Train: 48.081% | Validation: 45.647%\n",
      "Time elapsed: 9.78 min\n",
      "Epoch: 021/050 | Batch 0000/3882 | Cost: 2.0341\n",
      "Epoch: 021/050 | Batch 0500/3882 | Cost: 2.4827\n",
      "Epoch: 021/050 | Batch 1000/3882 | Cost: 2.2709\n",
      "Epoch: 021/050 | Batch 1500/3882 | Cost: 2.1068\n",
      "Epoch: 021/050 | Batch 2000/3882 | Cost: 2.3831\n",
      "Epoch: 021/050 | Batch 2500/3882 | Cost: 2.1817\n",
      "Epoch: 021/050 | Batch 3000/3882 | Cost: 1.9218\n",
      "Epoch: 021/050 | Batch 3500/3882 | Cost: 2.8572\n",
      "Epoch: 021/050 | Train: 48.114% | Validation: 45.599%\n",
      "Time elapsed: 10.27 min\n",
      "Epoch: 022/050 | Batch 0000/3882 | Cost: 2.3813\n",
      "Epoch: 022/050 | Batch 0500/3882 | Cost: 2.6024\n",
      "Epoch: 022/050 | Batch 1000/3882 | Cost: 2.2087\n",
      "Epoch: 022/050 | Batch 1500/3882 | Cost: 2.6679\n",
      "Epoch: 022/050 | Batch 2000/3882 | Cost: 2.2245\n",
      "Epoch: 022/050 | Batch 2500/3882 | Cost: 2.5176\n",
      "Epoch: 022/050 | Batch 3000/3882 | Cost: 2.4973\n",
      "Epoch: 022/050 | Batch 3500/3882 | Cost: 2.2398\n",
      "Epoch: 022/050 | Train: 48.441% | Validation: 46.076%\n",
      "Time elapsed: 10.75 min\n",
      "Epoch: 023/050 | Batch 0000/3882 | Cost: 2.3750\n",
      "Epoch: 023/050 | Batch 0500/3882 | Cost: 2.3027\n",
      "Epoch: 023/050 | Batch 1000/3882 | Cost: 2.1917\n",
      "Epoch: 023/050 | Batch 1500/3882 | Cost: 2.4431\n",
      "Epoch: 023/050 | Batch 2000/3882 | Cost: 2.3549\n",
      "Epoch: 023/050 | Batch 2500/3882 | Cost: 1.7705\n",
      "Epoch: 023/050 | Batch 3000/3882 | Cost: 2.5115\n",
      "Epoch: 023/050 | Batch 3500/3882 | Cost: 3.4275\n",
      "Epoch: 023/050 | Train: 48.469% | Validation: 46.151%\n",
      "Time elapsed: 11.26 min\n",
      "Epoch: 024/050 | Batch 0000/3882 | Cost: 2.5717\n",
      "Epoch: 024/050 | Batch 0500/3882 | Cost: 2.3056\n",
      "Epoch: 024/050 | Batch 1000/3882 | Cost: 2.3190\n",
      "Epoch: 024/050 | Batch 1500/3882 | Cost: 2.1421\n",
      "Epoch: 024/050 | Batch 2000/3882 | Cost: 2.4113\n",
      "Epoch: 024/050 | Batch 2500/3882 | Cost: 2.8682\n",
      "Epoch: 024/050 | Batch 3000/3882 | Cost: 2.3633\n",
      "Epoch: 024/050 | Batch 3500/3882 | Cost: 1.9361\n",
      "Epoch: 024/050 | Train: 48.626% | Validation: 46.196%\n",
      "Time elapsed: 11.75 min\n",
      "Epoch: 025/050 | Batch 0000/3882 | Cost: 1.7850\n",
      "Epoch: 025/050 | Batch 0500/3882 | Cost: 2.3741\n",
      "Epoch: 025/050 | Batch 1000/3882 | Cost: 2.4577\n",
      "Epoch: 025/050 | Batch 1500/3882 | Cost: 2.6981\n",
      "Epoch: 025/050 | Batch 2000/3882 | Cost: 1.9340\n",
      "Epoch: 025/050 | Batch 2500/3882 | Cost: 1.8953\n",
      "Epoch: 025/050 | Batch 3000/3882 | Cost: 2.4730\n",
      "Epoch: 025/050 | Batch 3500/3882 | Cost: 2.6604\n",
      "Epoch: 025/050 | Train: 48.901% | Validation: 46.285%\n",
      "Time elapsed: 12.27 min\n",
      "Epoch: 026/050 | Batch 0000/3882 | Cost: 2.2319\n",
      "Epoch: 026/050 | Batch 0500/3882 | Cost: 2.5127\n",
      "Epoch: 026/050 | Batch 1000/3882 | Cost: 2.2248\n",
      "Epoch: 026/050 | Batch 1500/3882 | Cost: 2.0271\n",
      "Epoch: 026/050 | Batch 2000/3882 | Cost: 2.6845\n",
      "Epoch: 026/050 | Batch 2500/3882 | Cost: 2.5520\n",
      "Epoch: 026/050 | Batch 3000/3882 | Cost: 2.6389\n",
      "Epoch: 026/050 | Batch 3500/3882 | Cost: 2.9360\n",
      "Epoch: 026/050 | Train: 49.086% | Validation: 46.494%\n",
      "Time elapsed: 12.78 min\n",
      "Epoch: 027/050 | Batch 0000/3882 | Cost: 1.7820\n",
      "Epoch: 027/050 | Batch 0500/3882 | Cost: 2.4973\n",
      "Epoch: 027/050 | Batch 1000/3882 | Cost: 2.4237\n",
      "Epoch: 027/050 | Batch 1500/3882 | Cost: 2.4712\n",
      "Epoch: 027/050 | Batch 2000/3882 | Cost: 2.7209\n",
      "Epoch: 027/050 | Batch 2500/3882 | Cost: 1.8747\n",
      "Epoch: 027/050 | Batch 3000/3882 | Cost: 2.5335\n",
      "Epoch: 027/050 | Batch 3500/3882 | Cost: 2.3709\n",
      "Epoch: 027/050 | Train: 49.023% | Validation: 46.622%\n",
      "Time elapsed: 13.28 min\n",
      "Epoch: 028/050 | Batch 0000/3882 | Cost: 2.5331\n",
      "Epoch: 028/050 | Batch 0500/3882 | Cost: 1.9613\n",
      "Epoch: 028/050 | Batch 1000/3882 | Cost: 2.1884\n",
      "Epoch: 028/050 | Batch 1500/3882 | Cost: 2.5831\n",
      "Epoch: 028/050 | Batch 2000/3882 | Cost: 2.3809\n",
      "Epoch: 028/050 | Batch 2500/3882 | Cost: 2.6605\n",
      "Epoch: 028/050 | Batch 3000/3882 | Cost: 2.5953\n",
      "Epoch: 028/050 | Batch 3500/3882 | Cost: 2.7717\n",
      "Epoch: 028/050 | Train: 48.914% | Validation: 46.480%\n",
      "Time elapsed: 13.79 min\n",
      "Epoch: 029/050 | Batch 0000/3882 | Cost: 2.2926\n",
      "Epoch: 029/050 | Batch 0500/3882 | Cost: 2.2676\n",
      "Epoch: 029/050 | Batch 1000/3882 | Cost: 2.3634\n",
      "Epoch: 029/050 | Batch 1500/3882 | Cost: 2.2870\n",
      "Epoch: 029/050 | Batch 2000/3882 | Cost: 2.4355\n",
      "Epoch: 029/050 | Batch 2500/3882 | Cost: 2.2835\n",
      "Epoch: 029/050 | Batch 3000/3882 | Cost: 2.6500\n",
      "Epoch: 029/050 | Batch 3500/3882 | Cost: 2.1588\n",
      "Epoch: 029/050 | Train: 49.106% | Validation: 46.535%\n",
      "Time elapsed: 14.29 min\n",
      "Epoch: 030/050 | Batch 0000/3882 | Cost: 2.0451\n",
      "Epoch: 030/050 | Batch 0500/3882 | Cost: 2.2886\n",
      "Epoch: 030/050 | Batch 1000/3882 | Cost: 2.1792\n",
      "Epoch: 030/050 | Batch 1500/3882 | Cost: 2.5992\n",
      "Epoch: 030/050 | Batch 2000/3882 | Cost: 2.3511\n",
      "Epoch: 030/050 | Batch 2500/3882 | Cost: 2.1176\n",
      "Epoch: 030/050 | Batch 3000/3882 | Cost: 2.4689\n",
      "Epoch: 030/050 | Batch 3500/3882 | Cost: 2.0078\n",
      "Epoch: 030/050 | Train: 49.523% | Validation: 46.731%\n",
      "Time elapsed: 14.80 min\n",
      "Epoch: 031/050 | Batch 0000/3882 | Cost: 1.9835\n",
      "Epoch: 031/050 | Batch 0500/3882 | Cost: 2.2055\n",
      "Epoch: 031/050 | Batch 1000/3882 | Cost: 3.0910\n",
      "Epoch: 031/050 | Batch 1500/3882 | Cost: 2.6481\n",
      "Epoch: 031/050 | Batch 2000/3882 | Cost: 2.8271\n",
      "Epoch: 031/050 | Batch 2500/3882 | Cost: 2.5031\n",
      "Epoch: 031/050 | Batch 3000/3882 | Cost: 2.3634\n",
      "Epoch: 031/050 | Batch 3500/3882 | Cost: 2.1406\n",
      "Epoch: 031/050 | Train: 49.553% | Validation: 46.760%\n",
      "Time elapsed: 15.32 min\n",
      "Epoch: 032/050 | Batch 0000/3882 | Cost: 2.2812\n",
      "Epoch: 032/050 | Batch 0500/3882 | Cost: 1.8008\n",
      "Epoch: 032/050 | Batch 1000/3882 | Cost: 2.6352\n",
      "Epoch: 032/050 | Batch 1500/3882 | Cost: 2.4846\n",
      "Epoch: 032/050 | Batch 2000/3882 | Cost: 2.0744\n",
      "Epoch: 032/050 | Batch 2500/3882 | Cost: 2.2061\n",
      "Epoch: 032/050 | Batch 3000/3882 | Cost: 2.7113\n",
      "Epoch: 032/050 | Batch 3500/3882 | Cost: 2.8244\n",
      "Epoch: 032/050 | Train: 49.552% | Validation: 46.752%\n",
      "Time elapsed: 15.83 min\n",
      "Epoch: 033/050 | Batch 0000/3882 | Cost: 1.9427\n",
      "Epoch: 033/050 | Batch 0500/3882 | Cost: 1.9610\n",
      "Epoch: 033/050 | Batch 1000/3882 | Cost: 2.5430\n",
      "Epoch: 033/050 | Batch 1500/3882 | Cost: 2.0548\n",
      "Epoch: 033/050 | Batch 2000/3882 | Cost: 2.4275\n",
      "Epoch: 033/050 | Batch 2500/3882 | Cost: 2.4112\n",
      "Epoch: 033/050 | Batch 3000/3882 | Cost: 2.0792\n",
      "Epoch: 033/050 | Batch 3500/3882 | Cost: 2.0273\n",
      "Epoch: 033/050 | Train: 49.512% | Validation: 46.799%\n",
      "Time elapsed: 16.36 min\n",
      "Epoch: 034/050 | Batch 0000/3882 | Cost: 2.3402\n",
      "Epoch: 034/050 | Batch 0500/3882 | Cost: 2.2008\n",
      "Epoch: 034/050 | Batch 1000/3882 | Cost: 2.6397\n",
      "Epoch: 034/050 | Batch 1500/3882 | Cost: 2.4166\n",
      "Epoch: 034/050 | Batch 2000/3882 | Cost: 3.4039\n",
      "Epoch: 034/050 | Batch 2500/3882 | Cost: 2.3876\n",
      "Epoch: 034/050 | Batch 3000/3882 | Cost: 2.4190\n",
      "Epoch: 034/050 | Batch 3500/3882 | Cost: 1.9403\n",
      "Epoch: 034/050 | Train: 49.560% | Validation: 46.688%\n",
      "Time elapsed: 16.86 min\n",
      "Epoch: 035/050 | Batch 0000/3882 | Cost: 2.4528\n",
      "Epoch: 035/050 | Batch 0500/3882 | Cost: 2.5690\n",
      "Epoch: 035/050 | Batch 1000/3882 | Cost: 2.2095\n",
      "Epoch: 035/050 | Batch 1500/3882 | Cost: 2.2002\n",
      "Epoch: 035/050 | Batch 2000/3882 | Cost: 2.6473\n",
      "Epoch: 035/050 | Batch 2500/3882 | Cost: 1.9744\n",
      "Epoch: 035/050 | Batch 3000/3882 | Cost: 2.6304\n",
      "Epoch: 035/050 | Batch 3500/3882 | Cost: 2.6829\n",
      "Epoch: 035/050 | Train: 49.555% | Validation: 46.736%\n",
      "Time elapsed: 17.38 min\n",
      "Epoch: 036/050 | Batch 0000/3882 | Cost: 2.1922\n",
      "Epoch: 036/050 | Batch 0500/3882 | Cost: 2.0619\n",
      "Epoch: 036/050 | Batch 1000/3882 | Cost: 1.8074\n",
      "Epoch: 036/050 | Batch 1500/3882 | Cost: 2.6495\n",
      "Epoch: 036/050 | Batch 2000/3882 | Cost: 2.3993\n",
      "Epoch: 036/050 | Batch 2500/3882 | Cost: 2.2276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 036/050 | Batch 3000/3882 | Cost: 2.4265\n",
      "Epoch: 036/050 | Batch 3500/3882 | Cost: 2.2319\n",
      "Epoch: 036/050 | Train: 49.727% | Validation: 46.969%\n",
      "Time elapsed: 17.88 min\n",
      "Epoch: 037/050 | Batch 0000/3882 | Cost: 2.0551\n",
      "Epoch: 037/050 | Batch 0500/3882 | Cost: 2.0063\n",
      "Epoch: 037/050 | Batch 1000/3882 | Cost: 2.2149\n",
      "Epoch: 037/050 | Batch 1500/3882 | Cost: 1.7955\n",
      "Epoch: 037/050 | Batch 2000/3882 | Cost: 2.1241\n",
      "Epoch: 037/050 | Batch 2500/3882 | Cost: 1.9355\n",
      "Epoch: 037/050 | Batch 3000/3882 | Cost: 2.8716\n",
      "Epoch: 037/050 | Batch 3500/3882 | Cost: 1.8470\n",
      "Epoch: 037/050 | Train: 49.334% | Validation: 46.631%\n",
      "Time elapsed: 18.38 min\n",
      "Epoch: 038/050 | Batch 0000/3882 | Cost: 2.1237\n",
      "Epoch: 038/050 | Batch 0500/3882 | Cost: 2.3148\n",
      "Epoch: 038/050 | Batch 1000/3882 | Cost: 2.5067\n",
      "Epoch: 038/050 | Batch 1500/3882 | Cost: 2.3305\n",
      "Epoch: 038/050 | Batch 2000/3882 | Cost: 2.5184\n",
      "Epoch: 038/050 | Batch 2500/3882 | Cost: 2.3125\n",
      "Epoch: 038/050 | Batch 3000/3882 | Cost: 2.2501\n",
      "Epoch: 038/050 | Batch 3500/3882 | Cost: 2.6437\n",
      "Epoch: 038/050 | Train: 49.702% | Validation: 46.899%\n",
      "Time elapsed: 18.87 min\n",
      "Epoch: 039/050 | Batch 0000/3882 | Cost: 1.9450\n",
      "Epoch: 039/050 | Batch 0500/3882 | Cost: 2.3263\n",
      "Epoch: 039/050 | Batch 1000/3882 | Cost: 2.2815\n",
      "Epoch: 039/050 | Batch 1500/3882 | Cost: 2.0495\n",
      "Epoch: 039/050 | Batch 2000/3882 | Cost: 2.4305\n",
      "Epoch: 039/050 | Batch 2500/3882 | Cost: 2.4416\n",
      "Epoch: 039/050 | Batch 3000/3882 | Cost: 2.0247\n",
      "Epoch: 039/050 | Batch 3500/3882 | Cost: 2.3377\n",
      "Epoch: 039/050 | Train: 49.552% | Validation: 46.733%\n",
      "Time elapsed: 19.35 min\n",
      "Epoch: 040/050 | Batch 0000/3882 | Cost: 2.3207\n",
      "Epoch: 040/050 | Batch 0500/3882 | Cost: 2.0506\n",
      "Epoch: 040/050 | Batch 1000/3882 | Cost: 2.4008\n",
      "Epoch: 040/050 | Batch 1500/3882 | Cost: 2.5007\n",
      "Epoch: 040/050 | Batch 2000/3882 | Cost: 2.5531\n",
      "Epoch: 040/050 | Batch 2500/3882 | Cost: 1.9266\n",
      "Epoch: 040/050 | Batch 3000/3882 | Cost: 2.3370\n",
      "Epoch: 040/050 | Batch 3500/3882 | Cost: 2.7154\n",
      "Epoch: 040/050 | Train: 49.966% | Validation: 47.093%\n",
      "Time elapsed: 19.83 min\n",
      "Epoch: 041/050 | Batch 0000/3882 | Cost: 2.4434\n",
      "Epoch: 041/050 | Batch 0500/3882 | Cost: 2.2568\n",
      "Epoch: 041/050 | Batch 1000/3882 | Cost: 2.3847\n",
      "Epoch: 041/050 | Batch 1500/3882 | Cost: 2.5856\n",
      "Epoch: 041/050 | Batch 2000/3882 | Cost: 2.3482\n",
      "Epoch: 041/050 | Batch 2500/3882 | Cost: 2.4112\n",
      "Epoch: 041/050 | Batch 3000/3882 | Cost: 2.2080\n",
      "Epoch: 041/050 | Batch 3500/3882 | Cost: 2.8765\n",
      "Epoch: 041/050 | Train: 49.872% | Validation: 46.969%\n",
      "Time elapsed: 20.32 min\n",
      "Epoch: 042/050 | Batch 0000/3882 | Cost: 2.3769\n",
      "Epoch: 042/050 | Batch 0500/3882 | Cost: 2.1530\n",
      "Epoch: 042/050 | Batch 1000/3882 | Cost: 2.4463\n",
      "Epoch: 042/050 | Batch 1500/3882 | Cost: 2.5826\n",
      "Epoch: 042/050 | Batch 2000/3882 | Cost: 2.4648\n",
      "Epoch: 042/050 | Batch 2500/3882 | Cost: 2.1338\n",
      "Epoch: 042/050 | Batch 3000/3882 | Cost: 2.7734\n",
      "Epoch: 042/050 | Batch 3500/3882 | Cost: 2.1478\n",
      "Epoch: 042/050 | Train: 49.941% | Validation: 47.081%\n",
      "Time elapsed: 20.81 min\n",
      "Epoch: 043/050 | Batch 0000/3882 | Cost: 2.4329\n",
      "Epoch: 043/050 | Batch 0500/3882 | Cost: 2.1911\n",
      "Epoch: 043/050 | Batch 1000/3882 | Cost: 2.0068\n",
      "Epoch: 043/050 | Batch 1500/3882 | Cost: 2.3593\n",
      "Epoch: 043/050 | Batch 2000/3882 | Cost: 1.9356\n",
      "Epoch: 043/050 | Batch 2500/3882 | Cost: 2.2764\n",
      "Epoch: 043/050 | Batch 3000/3882 | Cost: 2.3425\n",
      "Epoch: 043/050 | Batch 3500/3882 | Cost: 2.6837\n",
      "Epoch: 043/050 | Train: 50.061% | Validation: 47.150%\n",
      "Time elapsed: 21.30 min\n",
      "Epoch: 044/050 | Batch 0000/3882 | Cost: 2.4525\n",
      "Epoch: 044/050 | Batch 0500/3882 | Cost: 1.8360\n",
      "Epoch: 044/050 | Batch 1000/3882 | Cost: 2.4075\n",
      "Epoch: 044/050 | Batch 1500/3882 | Cost: 1.7474\n",
      "Epoch: 044/050 | Batch 2000/3882 | Cost: 2.4208\n",
      "Epoch: 044/050 | Batch 2500/3882 | Cost: 2.8225\n",
      "Epoch: 044/050 | Batch 3000/3882 | Cost: 2.3882\n",
      "Epoch: 044/050 | Batch 3500/3882 | Cost: 3.1278\n",
      "Epoch: 044/050 | Train: 49.746% | Validation: 46.944%\n",
      "Time elapsed: 21.79 min\n",
      "Epoch: 045/050 | Batch 0000/3882 | Cost: 2.3726\n",
      "Epoch: 045/050 | Batch 0500/3882 | Cost: 2.4987\n",
      "Epoch: 045/050 | Batch 1000/3882 | Cost: 2.2697\n",
      "Epoch: 045/050 | Batch 1500/3882 | Cost: 2.2834\n",
      "Epoch: 045/050 | Batch 2000/3882 | Cost: 2.7402\n",
      "Epoch: 045/050 | Batch 2500/3882 | Cost: 2.6169\n",
      "Epoch: 045/050 | Batch 3000/3882 | Cost: 2.5602\n",
      "Epoch: 045/050 | Batch 3500/3882 | Cost: 2.3669\n",
      "Epoch: 045/050 | Train: 50.136% | Validation: 47.333%\n",
      "Time elapsed: 22.28 min\n",
      "Epoch: 046/050 | Batch 0000/3882 | Cost: 2.4372\n",
      "Epoch: 046/050 | Batch 0500/3882 | Cost: 1.9109\n",
      "Epoch: 046/050 | Batch 1000/3882 | Cost: 2.7470\n",
      "Epoch: 046/050 | Batch 1500/3882 | Cost: 2.7622\n",
      "Epoch: 046/050 | Batch 2000/3882 | Cost: 2.1716\n",
      "Epoch: 046/050 | Batch 2500/3882 | Cost: 2.2304\n",
      "Epoch: 046/050 | Batch 3000/3882 | Cost: 2.4870\n",
      "Epoch: 046/050 | Batch 3500/3882 | Cost: 1.9966\n",
      "Epoch: 046/050 | Train: 49.964% | Validation: 47.250%\n",
      "Time elapsed: 22.78 min\n",
      "Epoch: 047/050 | Batch 0000/3882 | Cost: 2.1870\n",
      "Epoch: 047/050 | Batch 0500/3882 | Cost: 2.7136\n",
      "Epoch: 047/050 | Batch 1000/3882 | Cost: 2.6488\n",
      "Epoch: 047/050 | Batch 1500/3882 | Cost: 2.1642\n",
      "Epoch: 047/050 | Batch 2000/3882 | Cost: 2.6280\n",
      "Epoch: 047/050 | Batch 2500/3882 | Cost: 2.0717\n",
      "Epoch: 047/050 | Batch 3000/3882 | Cost: 2.2004\n",
      "Epoch: 047/050 | Batch 3500/3882 | Cost: 2.5292\n",
      "Epoch: 047/050 | Train: 50.068% | Validation: 47.027%\n",
      "Time elapsed: 23.28 min\n",
      "Epoch: 048/050 | Batch 0000/3882 | Cost: 1.9560\n",
      "Epoch: 048/050 | Batch 0500/3882 | Cost: 2.1819\n",
      "Epoch: 048/050 | Batch 1000/3882 | Cost: 2.2336\n",
      "Epoch: 048/050 | Batch 1500/3882 | Cost: 2.8743\n",
      "Epoch: 048/050 | Batch 2000/3882 | Cost: 2.3933\n",
      "Epoch: 048/050 | Batch 2500/3882 | Cost: 2.0848\n",
      "Epoch: 048/050 | Batch 3000/3882 | Cost: 2.6517\n",
      "Epoch: 048/050 | Batch 3500/3882 | Cost: 2.5919\n",
      "Epoch: 048/050 | Train: 50.364% | Validation: 47.419%\n",
      "Time elapsed: 23.78 min\n",
      "Epoch: 049/050 | Batch 0000/3882 | Cost: 2.1741\n",
      "Epoch: 049/050 | Batch 0500/3882 | Cost: 1.8845\n",
      "Epoch: 049/050 | Batch 1000/3882 | Cost: 2.5492\n",
      "Epoch: 049/050 | Batch 1500/3882 | Cost: 2.8265\n",
      "Epoch: 049/050 | Batch 2000/3882 | Cost: 2.1531\n",
      "Epoch: 049/050 | Batch 2500/3882 | Cost: 2.7372\n",
      "Epoch: 049/050 | Batch 3000/3882 | Cost: 2.2947\n",
      "Epoch: 049/050 | Batch 3500/3882 | Cost: 2.5592\n",
      "Epoch: 049/050 | Train: 50.327% | Validation: 47.370%\n",
      "Time elapsed: 24.29 min\n",
      "Epoch: 050/050 | Batch 0000/3882 | Cost: 2.2626\n",
      "Epoch: 050/050 | Batch 0500/3882 | Cost: 2.3925\n",
      "Epoch: 050/050 | Batch 1000/3882 | Cost: 2.4007\n",
      "Epoch: 050/050 | Batch 1500/3882 | Cost: 2.4251\n",
      "Epoch: 050/050 | Batch 2000/3882 | Cost: 2.6410\n",
      "Epoch: 050/050 | Batch 2500/3882 | Cost: 2.5300\n",
      "Epoch: 050/050 | Batch 3000/3882 | Cost: 2.0505\n",
      "Epoch: 050/050 | Batch 3500/3882 | Cost: 2.3184\n",
      "Epoch: 050/050 | Train: 49.836% | Validation: 46.823%\n",
      "Time elapsed: 24.78 min\n",
      "Total Training Time: 24.78 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.float()\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 500:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%% | Validation: %.3f%%' % (\n",
    "              epoch+1, NUM_EPOCHS, \n",
    "              compute_accuracy(model, train_loader, device=DEVICE),\n",
    "              compute_accuracy(model, valid_loader, device=DEVICE) ))\n",
    "        \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 46.94%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
